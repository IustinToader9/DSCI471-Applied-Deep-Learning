{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5abbbe63",
   "metadata": {
    "id": "5abbbe63"
   },
   "source": [
    "# 　　　　Build your `Convolutional Neural Network` for Deep Learning\n",
    "# 　　　　　　　　on Handwritten Digits `Classification` Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a8095",
   "metadata": {
    "id": "e07a8095"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c78a92d",
   "metadata": {
    "id": "1c78a92d"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65107702",
   "metadata": {
    "id": "65107702"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c3cbb9",
   "metadata": {
    "id": "90c3cbb9"
   },
   "source": [
    "## Task Overview \n",
    "<img src=\"https://user-images.githubusercontent.com/68801296/88917938-4008f180-d286-11ea-8667-50027700e3ea.png\" width=\"500\">\n",
    "　　　　　　　　　　　　　　　　　　　　　　　　　　　　<strong>A Task Schema Diagram</strong>\n",
    "                                              \n",
    " \n",
    "You will resolve a `classification` task, which is to recognize by classifying the handwritten images of 10 digits into 10 categories, with a `Deep Learning model` of convolutional Neural Network (`CNN`). You will `design` the architecture of the network, `implement` with Python, and `train` your model on a large set of sample image data as a supervised learning procedure.\n",
    "\n",
    "The `TensorFlow 2.x` platform and its integrated version of `Keras API` (versus the traditional Keras Package) should be the handy tools for you to complete this task. Please take advantage of the official well supported TensorFlow & Keras [`Help Documentations`](https://www.tensorflow.org/api_docs/python/tf/keras)  for correct usages of the API and functions. \n",
    "\n",
    "Finally, you will `evaluate` your model's accuracy using test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd074c",
   "metadata": {
    "id": "a4cd074c"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b9375e",
   "metadata": {
    "id": "66b9375e"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9883b4ba",
   "metadata": {
    "id": "9883b4ba"
   },
   "source": [
    "## Setup Development Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a47d22f8",
   "metadata": {
    "id": "a47d22f8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 17:10:43.112099: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "## Import related Modules for TensorFlow framework and Keras API\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "## import the integrated keras from tensorflow platform  \n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers, optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec08005",
   "metadata": {
    "id": "5ec08005"
   },
   "source": [
    "## Prepare Dataset \n",
    "\n",
    "### The Image Dataset of MNIST Handwritten Digits - Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a9523a",
   "metadata": {
    "id": "15a9523a"
   },
   "source": [
    "The [`MNIST`](http://yann.lecun.com/exdb/mnist/) database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is widely used for training and testing Deep Learning Models.\n",
    "\n",
    "It is a dataset of `images` and corresponding `labels`. The images dataset contains small square images with pixels in grayscale, and each image present a handwritten single digits between 0 and 9. The corresponding labels indicate the actual digits written in the images.\n",
    "\n",
    "The Classification task is to identify and inclusively classify a given image of a handwritten digit into one of 10 classes representing integer digit from 0 to 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cca8177",
   "metadata": {
    "id": "7cca8177"
   },
   "source": [
    "### Load Dataset\n",
    "\n",
    "Load the MNIST dataset distributed with Keras.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a4ac2f1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2a4ac2f1",
    "outputId": "ccb846a0-d11d-4d39-8195-6c960026248c"
   },
   "outputs": [],
   "source": [
    "## Load the MNIST dataset \n",
    "mnist_dataset = keras.datasets.mnist\n",
    "\n",
    "## Split MNIST dataset to get the images and labels for both train and test sets \n",
    "(images_train, labels_train),(images_test, labels_test) = mnist_dataset.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb690f",
   "metadata": {
    "id": "a4fb690f"
   },
   "source": [
    "###  Explore Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc96015",
   "metadata": {
    "id": "3fc96015"
   },
   "source": [
    "The MNIST dataset loaded and splited from Keras returns 4 [`NumPy arrays`](https://www.tutorialspoint.com/numpy/numpy_array_attributes.htm) that are assigned to 4 variables respectively:\n",
    "\n",
    "- The images_train and labels_train arrays are the training dataset for your model to learn;\n",
    "\n",
    "- You will need the images_test and labels_test arrays to test/evaluate your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7093ad",
   "metadata": {
    "id": "2f7093ad"
   },
   "source": [
    "##### Find out dimensions of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a3909d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91a3909d",
    "outputId": "b01fc230-4111-4bb4-e274-95417d4bd200",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of train dataset: \n",
      " (60000, 28, 28)\n",
      "Dimension of test dataset: \n",
      " (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "images_train_dimension = images_train.shape   \n",
    "print(\"Dimension of train dataset: \\n\", images_train_dimension)\n",
    "\n",
    "images_test_dimension = images_test.shape   \n",
    "print(\"Dimension of test dataset: \\n\", images_test_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e04cd62",
   "metadata": {
    "id": "1e04cd62"
   },
   "source": [
    "##### Find out the range of pixel values in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee50e3fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee50e3fb",
    "outputId": "a35ff3bd-63eb-4767-8a18-db7d5d42d729",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The range of pixel values of the images is: \n",
      " [0, 255]\n"
     ]
    }
   ],
   "source": [
    "print( \"The range of pixel values of the images is: \\n\", [np.min(images_train), np.max(images_train)] )   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b9364a",
   "metadata": {
    "id": "59b9364a"
   },
   "source": [
    "##### Find out the value of labels and their frequencies  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "277d4b7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "277d4b7f",
    "outputId": "f1793895-49b9-4922-cacf-e184b60f4513"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique values of the labels in the dataset are: \n",
      " [0 1 2 3 4 5 6 7 8 9]\n",
      "The count of each label in the dataset are: \n",
      " [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n"
     ]
    }
   ],
   "source": [
    "(labels, counts) = np.unique(labels_train, return_counts=True)\n",
    "print( \"The unique values of the labels in the dataset are: \\n\", labels )\n",
    "print( \"The count of each label in the dataset are: \\n\", counts )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120360dd",
   "metadata": {
    "id": "120360dd"
   },
   "source": [
    "##### Display a sample image \n",
    "get an idea of the kind of images your CNN is going to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb0f2d2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "fb0f2d2a",
    "outputId": "59d89f2c-95bd-4e8a-fae4-8ad528c5a8e4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image of the 6th in train dataset is displayed below: \n",
      "\n",
      "The label for the second training image is: \n",
      " 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Display the 6th image from the train dataset \n",
    "\n",
    "idx_img = 5\n",
    "\n",
    "print(\"The image of the 6th in train dataset is displayed below: \\n\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.gray()   ## Set the colormap for showing the image as grayscale\n",
    "plt.imshow(images_train[idx_img])\n",
    "\n",
    "print(\"The label for the second training image is: \\n\", labels_train[idx_img])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbc8b5",
   "metadata": {
    "id": "04bbc8b5"
   },
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f084a6",
   "metadata": {
    "id": "69f084a6"
   },
   "source": [
    "Create some functions that can be used to process the dataset as demanded before feed it in to train the network parameters.  \n",
    "\n",
    "The tensor of shape of an image's dimension is more formally described as (image_height, image_width, color_channels) instead of just (image_height, image_width), where color_channels refers to (R,G,B). For example, for one color image that has 3 color channels, its dimension would be (image_height, image_width, 3) \n",
    "\n",
    "Since the MNIST dataset contains images all in grayscale, so we can explicitly express that in its dimension for model training, by reshaping and adding the number of channels.\n",
    "\n",
    "We also need to map the image pixel value range from its original to the range of 0 to 1 in the float32 type to facilitate the parameter computations.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dc1f5f4",
   "metadata": {
    "id": "4dc1f5f4"
   },
   "outputs": [],
   "source": [
    "## Define a function that convert and rescale values of images and labels        \n",
    "\n",
    "def grayscale_converter(image, label):\n",
    "    \n",
    "    image = tf.cast(image, tf.float32)    ## Convert to enforce the values of image in floating-point number data type\n",
    "    label = tf.cast(label, tf.int64)      ## Convert to enforce the values of label in integer number data type\n",
    "    \n",
    "    image = image / 255    ## Rescale value of image pixels to a range of [0, 1] as gray level\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ca63f5c",
   "metadata": {
    "id": "8ca63f5c"
   },
   "outputs": [],
   "source": [
    "## Define a function that enforce the image color channel for grayscale image, \n",
    "##  and randomly sample the amount from preprocessed images and labels to prepare dataset that suit for the model        \n",
    "\n",
    "def preprocess_dataset(images, labels, count, batch_size):       \n",
    "    \n",
    "    ## Convert dimension of all images to enforce the grayscale of the dataset that has only one color channel\n",
    "    images_dimension = images.shape\n",
    "    images = images.reshape(images_dimension[0], images_dimension[1], images_dimension[2], 1)        \n",
    "     \n",
    "    ## Prepare dataset by packing randomly sampled and batched images and lables\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))    \n",
    "    dataset = dataset.map(grayscale_converter)    ## rescale images in dataset to a range of 0 to 1\n",
    "    dataset = dataset.take(count).shuffle(count).batch(batch_size)  ## randomly sample and batch the dataset\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaeb97b",
   "metadata": {
    "id": "cdaeb97b"
   },
   "source": [
    "## Build architecture of your CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b8acab",
   "metadata": {
    "id": "72b8acab"
   },
   "source": [
    "The basic building block of a neural network is the layer.\n",
    "\n",
    "You will configure each layer of your model, then compiling the model.\n",
    "\n",
    "\n",
    "\n",
    "For this task you are required to chain together following layers as the architecture of the CNN:\n",
    "\n",
    "   - 2 [Convolutional layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) with [Max Pooling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) for spatial hierarchy,\n",
    "\n",
    "   - 1 [Flatten layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten), and \n",
    "\n",
    "   - 2 [Dense layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) that end with a `Tanh` and `Softmax` activation respectively\n",
    "\n",
    "So in the end you get a multiclass probability distribution as the output from your model to classify the digit written in an image.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef49ed",
   "metadata": {
    "id": "2bef49ed"
   },
   "source": [
    "##### Stack and Configure layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "361e5770",
   "metadata": {
    "id": "361e5770"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 17:24:50.832634: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Build a tf.keras.Sequential model by stacking layers\n",
    "\n",
    "model = keras.Sequential([     \n",
    "\n",
    "    layers.Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu', padding='same') , \n",
    "    \n",
    "    layers.MaxPool2D(pool_size=2, strides=2),    \n",
    "    \n",
    "    layers.Conv2D(64, (5, 5), activation='relu', padding='same') ,\n",
    "    \n",
    "    layers.MaxPool2D(pool_size=2, strides=2) ,\n",
    "    \n",
    "    layers.Flatten(),         \n",
    "\n",
    "    layers.Dense(128, activation='tanh'),    \n",
    "    \n",
    "    layers.Dense(10, activation='softmax')  \n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b2fcb3",
   "metadata": {
    "id": "09b2fcb3"
   },
   "source": [
    "##### Display the architecture of your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f99b7808",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f99b7808",
    "outputId": "d9c0c025-164e-476e-e08f-174a0ef2b296",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        832       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 64)        51264     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               401536    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 454,922\n",
      "Trainable params: 454,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e6851",
   "metadata": {
    "id": "783e6851"
   },
   "source": [
    "##### Compile the model\n",
    "\n",
    "Before the model is ready for training, it needs a few more settings. These are to be added during the [model's compiling](https://www.tensorflow.org/api_docs/python/tf/keras/Model) step:\n",
    "\n",
    "- [Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) : This is how the model is updated based on the data it sees and its loss function.\n",
    "- [Loss function](https://www.tensorflow.org/api_docs/python/tf/keras/losses) : This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n",
    "- [Metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics) : Used to monitor the training and testing steps.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6fb8782",
   "metadata": {
    "id": "b6fb8782"
   },
   "outputs": [],
   "source": [
    "my_optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=my_optimizer,\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy', 'mse']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e3703",
   "metadata": {
    "id": "567e3703"
   },
   "source": [
    "## Fit/Train the model\n",
    "\n",
    "Train the model to fit the train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e029773",
   "metadata": {
    "id": "3e029773"
   },
   "source": [
    "##### Obtain train dataset\n",
    "\n",
    "Apply the function previously defined for dataset preprocessing \n",
    "\n",
    "to Randomly sample 20,000 images and labels form the default train dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f146eec",
   "metadata": {
    "id": "6f146eec"
   },
   "outputs": [],
   "source": [
    "## set required number of samples to be randomly taken from the original train dataset\n",
    "train_sample_count = 20000\n",
    "\n",
    "## Decide a proper value of batch size for your model training \n",
    "train_batch_size = 25   \n",
    "\n",
    "## Obtain the train dataset prepared using previously defined function\n",
    "dataset_train = preprocess_dataset(images_train, labels_train, train_sample_count, train_batch_size)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10273401",
   "metadata": {
    "id": "10273401"
   },
   "source": [
    "##### Train your model\n",
    "Feed the train dataset to the model and let it  [fit to associate images and labels](https://www.tensorflow.org/api_docs/python/tf/keras/Model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f0d2fa6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f0d2fa6",
    "outputId": "9e5f9f6f-b4e5-4a1d-a9e5-cfd2bede4007",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "800/800 [==============================] - 20s 24ms/step - loss: 0.1680 - accuracy: 0.9496 - mse: 27.3924\n",
      "Epoch 2/12\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 0.0468 - accuracy: 0.9846 - mse: 27.3988\n",
      "Epoch 3/12\n",
      "800/800 [==============================] - 19s 23ms/step - loss: 0.0261 - accuracy: 0.9912 - mse: 27.3998\n",
      "Epoch 4/12\n",
      "800/800 [==============================] - 19s 23ms/step - loss: 0.0190 - accuracy: 0.9940 - mse: 27.4003\n",
      "Epoch 5/12\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 0.0108 - accuracy: 0.9963 - mse: 27.4007\n",
      "Epoch 6/12\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 0.0083 - accuracy: 0.9974 - mse: 27.4009\n",
      "Epoch 7/12\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 0.0054 - accuracy: 0.9985 - mse: 27.4011\n",
      "Epoch 8/12\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 0.0104 - accuracy: 0.9967 - mse: 27.4009\n",
      "Epoch 9/12\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 0.0061 - accuracy: 0.9980 - mse: 27.4010\n",
      "Epoch 10/12\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 0.0016 - accuracy: 0.9997 - mse: 27.4014\n",
      "Epoch 11/12\n",
      "800/800 [==============================] - 20s 25ms/step - loss: 4.6272e-04 - accuracy: 0.9999 - mse: 27.4015\n",
      "Epoch 12/12\n",
      "800/800 [==============================] - 19s 24ms/step - loss: 1.7950e-04 - accuracy: 0.9999 - mse: 27.4015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9a3b7f9ed0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Decide a proper value of epoch for your model training \n",
    "epochs_count = 12\n",
    "\n",
    "## Train model\n",
    "model.fit(dataset_train, epochs=epochs_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832515a4",
   "metadata": {
    "id": "832515a4"
   },
   "source": [
    "## Tuning your Model\n",
    "\n",
    "Go back to adjust the values of hyperparameters such as `learning rate`, `batch size`, and `epochs` in your aboce codes, to improve and achieve an **accuracy more than 97%**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40874051",
   "metadata": {
    "id": "40874051"
   },
   "source": [
    "## Evaluate/Test the trained model\n",
    "\n",
    "Evaluates your trained model based on the test dataset to see how the model performs on the test dataset after being trained to reach a good looking accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accc735f",
   "metadata": {
    "id": "accc735f"
   },
   "source": [
    "##### Obtain test dataset\n",
    "\n",
    "Apply the function previously defined for dataset preprocessing \n",
    "\n",
    "to randomly sample ALL number of images and labels form the `Original test dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "162ef7a3",
   "metadata": {
    "id": "162ef7a3"
   },
   "outputs": [],
   "source": [
    "## Set required number of samples to be randomly taken from the original test dataset\n",
    "test_sample_count = 10000 \n",
    "\n",
    "## Obtain the test dataset prepared using previously defined function\n",
    "dataset_test = preprocess_dataset(images_test, labels_test, test_sample_count, test_sample_count)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f3c728",
   "metadata": {
    "id": "d2f3c728"
   },
   "source": [
    "##### evaluate your trained model\n",
    "Feed the obtained test dataset to the model and   [evaluate the accuracy](https://www.tensorflow.org/api_docs/python/tf/keras/Model)   on the test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce608f93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce608f93",
    "outputId": "87732b3c-a266-420d-bd5b-125522ffdb63",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.0334 - accuracy: 0.9918 - mse: 27.3398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.033389050513505936, 0.9918000102043152, 27.339780807495117]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Evaluate model\n",
    "model.evaluate(dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe4292",
   "metadata": {
    "id": "60fe4292"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d7ce12",
   "metadata": {
    "id": "38d7ce12"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4cdfdc",
   "metadata": {
    "id": "df4cdfdc"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1999f6f6",
   "metadata": {
    "id": "1999f6f6"
   },
   "source": [
    "## Make prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1a9c8",
   "metadata": {
    "id": "69d1a9c8"
   },
   "source": [
    "Pick one image from the MNIST dataset to see what digit your trained model predict, as it being classified to which of the 10 classes (0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ade29557",
   "metadata": {
    "id": "ade29557"
   },
   "outputs": [],
   "source": [
    "## Pick the 2nd image from the original test images to feed into the trained model\n",
    "\n",
    "idx_img_predict = 1\n",
    "    \n",
    "image_predict = images_test[idx_img_predict]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516a2136",
   "metadata": {
    "id": "516a2136"
   },
   "source": [
    "Preporcess the picked image with the same procedure applied on train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ad198f4",
   "metadata": {
    "id": "3ad198f4"
   },
   "outputs": [],
   "source": [
    "## Convert image dimension to explicitly indicate it has only one sample and only one color channel\n",
    "image_predict = image_predict.reshape((1,) + image_predict.shape + (1,)) \n",
    "\n",
    "## Convert image pixel values to floating-point number and rescale to [0,1]\n",
    "image_predict = tf.cast(image_predict, tf.float32) / 255    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918df7f8",
   "metadata": {
    "id": "918df7f8"
   },
   "source": [
    "Feed the preprocessed image to the trained model to   [predict a result](https://www.tensorflow.org/api_docs/python/tf/keras/Model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb200401",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb200401",
    "outputId": "200092bd-af4f-4bf2-82f2-938417ca6cd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 92ms/step\n",
      "prediction result is: \n",
      " [[8.5429690e-08 5.8301398e-06 9.9999392e-01 1.7752370e-08 1.4232463e-09\n",
      "  2.8234184e-11 4.3223359e-08 2.0273806e-08 6.4265677e-08 2.8386365e-10]]\n"
     ]
    }
   ],
   "source": [
    "## Predict on the picked image \n",
    "prediction = model.predict(image_predict)\n",
    "\n",
    "print(\"prediction result is: \\n\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac529ad",
   "metadata": {
    "id": "eac529ad"
   },
   "source": [
    "[Interpret](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) the prediction by processing the multiclass result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90d3b193",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90d3b193",
    "outputId": "88926548-83cd-4bec-93ea-3d75c6aad119"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted digit number from this image is: \n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "## Interpret the output from the Softmax activation function of the model to see what digit is predicted by your model \n",
    "print(\"The predicted digit number from this image is: \\n\", np.argmax(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca5618",
   "metadata": {
    "id": "c4ca5618"
   },
   "source": [
    "##### Compare the prediction with the provided data\n",
    "\n",
    "Display the picked image of prediction to have a visual comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e8ce62b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "8e8ce62b",
    "outputId": "8c3ffd2b-d009-4181-a91d-3071a547ffe3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9a2b9d1f30>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Display the 2nd image from the original test images  \n",
    "plt.imshow(images_test[idx_img_predict])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c22c298",
   "metadata": {
    "id": "9c22c298"
   },
   "source": [
    "Compare the provided label from MNIST dataset associated with this image of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e21b49b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e21b49b6",
    "outputId": "3c77152f-27ee-4b75-fdee-989c908d65ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided label for this image is: \n",
      " 2\n"
     ]
    }
   ],
   "source": [
    " print(\"The provided label for this image is: \\n\", labels_test[idx_img_predict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35032349",
   "metadata": {
    "id": "35032349"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:TensorFlow2] *",
   "language": "python",
   "name": "conda-env-TensorFlow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
